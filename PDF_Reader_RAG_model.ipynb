{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cadb0873-126f-41aa-ad36-d196c5ecd8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform==1.35.0 in ./.local/lib/python3.10/site-packages (1.35.0)\n",
      "Requirement already satisfied: google-cloud-documentai==2.20.1 in ./.local/lib/python3.10/site-packages (2.20.1)\n",
      "Requirement already satisfied: backoff==2.2.1 in /opt/conda/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (3.19.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (23.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (2.13.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (3.13.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (1.11.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.35.0) (2.0.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.62.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (2.25.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.48.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.48.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.35.0) (0.12.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform==1.35.0) (1.24.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (4.9)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.10/site-packages (from grpcio<2.0dev,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (2023.11.17)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade google-cloud-aiplatform==1.35.0 google-cloud-documentai==2.20.1 backoff==2.2.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaefa250-7577-416c-a4bb-4168335c74c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "else:\n",
    "    # Otherwise, attempt to discover local credentials as described on https://cloud.google.com/docs/authentication/application-default-credentials\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d62716da-00f7-4940-8682-7c13354d816d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import backoff\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.api_core.exceptions import AlreadyExists\n",
    "from google.cloud import documentai\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "from logging import error\n",
    "import re\n",
    "import textwrap\n",
    "from typing import Tuple, List\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel\n",
    "import fitz \n",
    "import json\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2c798-e51b-436b-bb56-702a3c7e807e",
   "metadata": {},
   "source": [
    "## Pre-configuration of GCP project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42bebf17-63d6-43cc-92e2-19a78556f68a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation \"operations/acat.p2-268250494950-ee35f94b-df81-4663-9a97-f3d26a27c6ae\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "#Once the project is created in the console, extract the parameters here\n",
    "PROJECT_ID = !gcloud config get project\n",
    "PROJECT_ID = PROJECT_ID.n\n",
    "LOCATION = \"europe-west2\"\n",
    "LOCATION_DEPLOY = \"europe-west2\" #Location to deploy GCP resources\n",
    "\n",
    "!gcloud services enable documentai.googleapis.com storage.googleapis.com aiplatform.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e5522f-f6b1-420e-b84f-1bff0aef13e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClientOptions: {'api_endpoint': 'europe-west2-documentai.googleapis.com', 'client_cert_source': None, 'client_encrypted_cert_source': None, 'quota_project_id': None, 'credentials_file': None, 'scopes': None, 'api_key': None, 'api_audience': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ce793-b170-4972-a35e-52b71cb9b9d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "The processor is now ready to be defined by calling the document AI API. Code source: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-qa/question_answering_documentai_vector_store_palm.ipynb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c82772-4c03-415e-b2dd-dca3b3cb51aa",
   "metadata": {},
   "source": [
    "## 1. Create the processor\n",
    "\n",
    "\n",
    "\"There are two types of Document AI processors:\n",
    "\n",
    "Pre-trained processors: These processors are pre-trained on a large dataset of documents and can be used to perform common document processing tasks, such as Optical Character Recognition (OCR), form parsing, and entity extraction.\n",
    "Custom processors: These processors can be trained on your own dataset of documents to perform specific tasks that are not covered by the pre-trained processors.\n",
    "Refer to Full processor and detail list for all supported processors.\n",
    "\n",
    "Processors take a PDF or image file as input and output the data in the Document format.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77d89047-ad83-4dcf-b773-c5123ea052ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Processor projects/268250494950/locations/europe-west2/processors/1553b92dcd6e0f0b\n"
     ]
    }
   ],
   "source": [
    "# Edit these variables before running the code.\n",
    "project_id = PROJECT_ID\n",
    "\n",
    "# See https://cloud.google.com/document-ai/docs/regions for all options.\n",
    "location = LOCATION\n",
    "\n",
    "# Must be unique per project, e.g.: \"My Processor\"\n",
    "processor_display_name = \"my_processor1\"\n",
    "\n",
    "# You must set the `api_endpoint` if you use a location other than \"us\".\n",
    "client_options = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "\n",
    "#1. Create the processor: you can not create multiple processors with the same display name\n",
    "def create_processor(\n",
    "    project_id: str, location: str, processor_display_name: str\n",
    ") -> documentai.Processor:\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the location\n",
    "    # e.g.: projects/project_id/locations/location\n",
    "    parent = client.common_location_path(project_id, location)\n",
    "\n",
    "    # Create a processor\n",
    "    return client.create_processor(\n",
    "        parent=parent,\n",
    "        processor=documentai.Processor(\n",
    "            display_name=processor_display_name, type_=\"OCR_PROCESSOR\" #we are using the pre-trained OCR processor\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    processor = create_processor(project_id, location, processor_display_name)\n",
    "    print(f\"Created Processor {processor.name}\")\n",
    "except AlreadyExists as e:\n",
    "    print(\n",
    "        f\"Processor already exits, change the processor name and rerun this code. {e.message}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "#2. Define process document function which takes the processor name and file path of the document and extracts the text from the document.  \n",
    "def process_document(\n",
    "    processor_name: str,\n",
    "    file_path: str,\n",
    ") -> documentai.Document:\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=client_options)\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Load Binary Data into Document AI RawDocument Object\n",
    "    raw_document = documentai.RawDocument(\n",
    "        content=image_content, mime_type=\"application/pdf\"\n",
    "    )\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(name=processor_name, raw_document=raw_document)\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    return result.document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a7af7a-de00-4d2b-8ed0-fc7170ca997a",
   "metadata": {},
   "source": [
    "The document AI processor set a limit of 15 pages of PDF to be processed at once. The following function will split the initial PDF into pdfs of 15 pages max. The created pdfs are saved under new folder pdf_chunks with path labelled 1 to n (n=number of pdf chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a997be51-6539-4b9f-9b19-09377620ee35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_and_save_pdf(input_pdf_path: str, max_pages_per_file: int):\n",
    "    # Create a folder to store the split PDFs\n",
    "    output_folder = os.path.join(os.path.dirname(input_pdf_path), 'pdf_chunks')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    pdf_paths = []\n",
    "\n",
    "    # Open the input PDF\n",
    "    with fitz.open(input_pdf_path) as pdf_document:\n",
    "        num_pages = pdf_document.page_count\n",
    "\n",
    "        # Calculate the number of files needed\n",
    "        num_files = (num_pages + max_pages_per_file - 1) // max_pages_per_file\n",
    "\n",
    "        # Split the PDF into multiple files\n",
    "        for i in range(num_files):\n",
    "            start_page = i * max_pages_per_file\n",
    "            end_page = min((i + 1) * max_pages_per_file, num_pages)\n",
    "\n",
    "            pdf_writer = fitz.open()\n",
    "            pdf_writer.insert_pdf(pdf_document, from_page=start_page, to_page=end_page - 1)\n",
    "\n",
    "            output_pdf_path = os.path.join(output_folder, f'pdf_{i + 1}.pdf')\n",
    "            pdf_writer.save(output_pdf_path)\n",
    "\n",
    "            print(f'Saved: {output_pdf_path}')\n",
    "            pdf_paths.append(output_pdf_path)\n",
    "        \n",
    "    \n",
    "    return pdf_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02816726-2f33-46e6-94ef-76dd3e77abc0",
   "metadata": {},
   "source": [
    "We are now ready to extract the text from our pdf. \n",
    "\n",
    "## 2. Text extraction from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "161ebf6c-63ed-4abb-a32f-388c4334c8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: pdf_chunks/pdf_1.pdf\n",
      "Saved: pdf_chunks/pdf_2.pdf\n",
      "Saved: pdf_chunks/pdf_3.pdf\n"
     ]
    }
   ],
   "source": [
    "# Set the desired parameters\n",
    "input_pdf_path = \"SC145746_aa_2021-10-29.pdf\" # Replace with your actual input PDF path\n",
    "max_pages_per_file = 15 # Set the desired maximum number of pages per file\n",
    "processor_name = processor.name # Assign the created processor name\n",
    "\n",
    "\n",
    "# 1. Split the PDF into pdf chunks of 15 pages max and save their paths \n",
    "pdf_paths = split_and_save_pdf(input_pdf_path, max_pages_per_file)\n",
    "\n",
    "\n",
    "# 2. Iterate through the pdf chunks and extract and join their text\n",
    "texts = []\n",
    "for pdf_path in pdf_paths:   \n",
    "    document = process_document(processor_name, file_path=pdf_path)\n",
    "    texts.append(document.text)\n",
    "    \n",
    "text = ''.join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efed1f-b943-45b4-b89e-840bac0bd172",
   "metadata": {},
   "source": [
    "3. Text chunking\n",
    "\n",
    "The extracted text from the PDF is chunked following these steps: \n",
    "\n",
    "    - Split into sentences\n",
    "    - Create paragraphs based on number of element per chunk which can be set up manually \n",
    "    - Create DataFrame where each row is a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff818bfc-d7c0-4fff-a529-def827132663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split into sentences\n",
    "def text_to_sentences(text):\n",
    "    sentences = text.split('\\n')\n",
    "    return sentences\n",
    "\n",
    "def create_sentence_chunks(sentences, chunk_size):\n",
    "    sentence_chunks = [sentences[i:i+chunk_size] for i in range(0, len(sentences), chunk_size)]\n",
    "    return sentence_chunks\n",
    "\n",
    "def text_to_paragraph(text, chunk_size):\n",
    "    return create_sentence_chunks(text_to_sentences(text), chunk_size)\n",
    "\n",
    "def paragraphs_to_df(paragraphs):\n",
    "    data = []\n",
    "\n",
    "    # Iterate through sentence chunks\n",
    "    for idx, chunk in enumerate(sentence_chunks, start=1):\n",
    "        # Merge sentences within the chunk into one string\n",
    "        merged_text = ''.join(chunk)\n",
    "        # Append data to the list\n",
    "        data.append({\n",
    "            'paragraph_number':idx,\n",
    "            'text':merged_text\n",
    "        })\n",
    "    # DataFrame with different paragraphs\n",
    "    return pd.DataFrame(data)\n",
    "    \n",
    "chunk_size = 40\n",
    "sentence_chunks = text_to_paragraph(text, chunk_size)\n",
    "df = paragraphs_to_df(sentence_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60676ba4-4565-4108-b1ce-d95387f7d44e",
   "metadata": {},
   "source": [
    "## 4. Create the embeddings\n",
    "\n",
    "For each paragraph in the DataFrame, the gecko embedding model from GCP is applied to create the corresponding embeddign vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8360991a-7f39-4d87-90e3-65dc71fa41ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the GCP models\n",
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@002\")\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@002\")\n",
    "\n",
    "\n",
    "# This decorator is used to handle exceptions and apply exponential backoff in case of ResourceExhausted errors.\n",
    "# It means the function will be retried with increasing time intervals in case of this specific exception.\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\n",
    "def text_generation_model_with_backoff(**kwargs):\n",
    "    return generation_model.predict(**kwargs).text\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    get_embedding.counter += 1\n",
    "    try:\n",
    "        if get_embedding.counter % 100 == 0:\n",
    "            time.sleep(3)\n",
    "        return embedding_model.get_embeddings([text])[0].values #Send request to embedding model\n",
    "    except:\n",
    "        print('waiting for 60 secs')\n",
    "        time.sleep(60)\n",
    "        return embedding_model.get_embeddings([text])[0].values #Send request to embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5b1beea-f56d-477d-9c38-4344a0d78416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_embedding.counter = 0\n",
    "\n",
    "# This may take several minutes to complete.\n",
    "df[\"embedding\"] = df[\"text\"].apply(lambda x: get_embedding(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401f917-d725-4ef1-962d-b3f2967af2a0",
   "metadata": {},
   "source": [
    "The same embedding model is applied on the question of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9deda000-4fcb-49bc-b440-9204c64b51d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_context_from_question(\n",
    "    question: str, vector_store: pd.DataFrame, sort_index_value: int = 3\n",
    ") -> Tuple[str, pd.DataFrame]:\n",
    "    query_vector = np.array(get_embedding(question))\n",
    "    vector_store[\"dot_product\"] = vector_store[\"embedding\"].apply(\n",
    "        lambda row: np.dot(row, query_vector)\n",
    "    )\n",
    "    # Similarity matching by dot product \n",
    "    top_matched = vector_store.sort_values(by=\"dot_product\", ascending=False)[\n",
    "        :sort_index_value\n",
    "    ].index\n",
    "    \n",
    "    top_matched_df = vector_store.loc[top_matched, [\"paragraph_number\", \"text\"]]\n",
    "    context = \"\\n\".join(top_matched_df[\"text\"].values)\n",
    "    \n",
    "    return context, top_matched_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ca9ef-b315-4393-bf13-bde75506473f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcf5e3ab-3689-4abf-b1f2-806fa81d25f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me the scope 1 emissions in 2021?\n",
      "PaLM Predicted:  The scope 1 emissions in 2021 were 240.00 metric tonnes. \n",
      "\n",
      "\n",
      "    paragraph_number                                               text\n",
      "30                31  3,573,204758,517383,67220,828,484Depreciation ...\n",
      "26                27  20212020££Interest on bank overdrafts and loan...\n",
      "5                  6  - Gas combustion740,145- Fuel consumed for tra...\n",
      "0                  1  Company Registration No. SC145746 (Scotland)TH...\n",
      "CPU times: user 18.5 ms, sys: 10.2 ms, total: 28.6 ms\n",
      "Wall time: 723 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# your question for the documents\n",
    "question = \"Give me the scope 1 emissions in 2021?\"\n",
    "\n",
    "# get the custom relevant chunks from all the chunks in vector store.\n",
    "context, top_matched_df = get_context_from_question(\n",
    "    question,\n",
    "    vector_store=df,\n",
    "    sort_index_value=4,  # Top N results to pick from embedding vector search\n",
    ")\n",
    "prompt = f\"\"\" Answer the question as precise as possible using the provided context. \\n\\n\n",
    "            Context: \\n {context}?\\n\n",
    "            Question: \\n {question} \\n\n",
    "            Answer:\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "# Call the PaLM API on the prompt.\n",
    "print(question)\n",
    "print(\"PaLM Predicted:\", text_generation_model_with_backoff(prompt=prompt), \"\\n\\n\")\n",
    "# top 5 data that has been picked by model based on user question. This becomes the context.\n",
    "print(top_matched_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ce4fb-2ae4-4da0-becd-61f6bf1dc671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
