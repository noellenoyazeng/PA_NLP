{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583261ab-24e9-4ca9-ac9e-433a40286ae6",
   "metadata": {},
   "source": [
    "# Prompt Design - Code (Google Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a65b3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook covers the essentials of prompt engineering for code generation.\n",
    "\n",
    "Learn more about prompt design in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview).\n",
    "\n",
    "### Language Support\n",
    "\n",
    "C, C++, C#, Clojure, Dart, Elixir, Erlang, Fortran, Go, GoogleSQL, Groovy, Haskell, Java, JavaScript, Kotlin, Lean (proof assistant), Objective-C, OCaml, Perl, PHP, Python, Ruby, Rust, Scala, Shell Script, Solidity, Swift, TypeScript, Verilog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddefa92",
   "metadata": {},
   "source": [
    "## Zero, One and Few Shot Prompting\n",
    "\n",
    "A way to improve response quality is to add examples in your prompt. The LLM learns in-context from the examples on how to respond. Typically, one to five examples (shots) are enough to improve the quality of responses. Including too many examples can cause the model to over-fit the data and reduce the quality of responses.\n",
    "\n",
    "Similar to classical model training, the quality and distribution of the examples is very important. Pick examples that are representative of the scenarios that you need the model to learn, and keep the distribution of the examples (e.g. number of examples per class in the case of classification) aligned with your actual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"shapely<2.0.0\"\n",
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bd836-4f86-4e19-99f2-390a2e17a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import CodeGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e317f86",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83e59b-7d6c-461d-ae52-73ce11954082",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = CodeGenerationModel.from_pretrained(\"code-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ef8f2-bbff-494f-83f1-e89b7fc68613",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt\n",
    "\n",
    "Below is an example of zero-shot prompting, where you don't provide any examples to the LLM within the prompt itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e0dee-ce53-41e7-aa83-546cbdecfd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generate a Python function that checks if a year is a leap year\"\n",
    "\n",
    "# Note that code models take prefix and suffix as opposed to prompt as parameters\n",
    "print(generation_model.predict(prefix=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbafac-fba0-46e0-af3b-7a6918092d28",
   "metadata": {},
   "source": [
    "### One-shot Prompt\n",
    "\n",
    "Below is an example of one-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceaed4c-d697-4871-9081-97dfc2702307",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Generate a unit test for a function\n",
    "\n",
    "function:\n",
    "def multiply_two_numbers(x,y):\n",
    "    return x*y\n",
    "unit test:\n",
    "def multiply_two_numbers_test(x,y):\n",
    "    assert multiply_two_numbers(5,2) == 10\n",
    "    assert multiply_two_numbers(10,1) == 10\n",
    "\n",
    "function:\n",
    "def is_leap_year(year):\n",
    "    if year % 4 == 0:\n",
    "        if year % 100 == 0:\n",
    "            if year % 400 == 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "unit test:\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prefix=prompt, max_output_tokens=256).text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37120f42-3e56-4037-8693-48b0acb4752f",
   "metadata": {},
   "source": [
    "### Few-Shot Prompt\n",
    "\n",
    "Below is an example of few-shot prompting, where you provide a few examples to the LLM within the prompt to give some guidance on what type of response you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe19eb6-f512-4b4b-b794-e1d971194748",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Convert plain text specifications into a Python dictionary. If multiple specifications fall under the same category, convert into an array\n",
    "\n",
    "plain text: Wifi Router, network speed up to 1200Mpbs, 2.4GHz and 5GHz frequencies, WP3 protocol\n",
    "product = {\n",
    "  \"product\":\"Wifi Router Model 1\",\n",
    "  \"network speed\":\"1200Mpbs\",\n",
    "  \"frequencies\": [\"2.4GHz\", \"5GHz\"],\n",
    "  \"protocol\":\"WP3\"\n",
    "}\n",
    "plain text: Phone Model 1,4G and 5G network, 8GB RAM, G2 processor, 128GB of storage, Lemongrass\n",
    "product = {\n",
    "  \"product\":\"Phone Model 1\",\n",
    "  \"network type\":[\"4G\",\"5G\"],\n",
    "  \"ram\":\"8GB\",\n",
    "  \"processor\":\"G2\",\n",
    "  \"storage\":\"128GB\",\n",
    "  \"color\":\"Lemongrass\"\n",
    "}\n",
    "plain text: Laptop Model 1, Wifi 6 Compatible, 32GB RAM, A100 GPU with 16GB Vram, 2TB SSD, 1TB HDD, Silver\n",
    "\"\"\"\n",
    "\n",
    "print(generation_model.predict(prefix=prompt, max_output_tokens=256).text)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
