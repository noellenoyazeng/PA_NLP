{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d4a25f-c7d0-43e8-8f29-0e201cdfcb34",
   "metadata": {},
   "source": [
    "# Deployment of Model Garden Open-Source Model (OpenLlama 3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf4a85",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook demonstrates deploying and running inference for a sample LLM from Model Garden (OpenLlama 3B). The functions in this notebook can be adapted for other models from Model Garden.\n",
    "\n",
    "[openlm-research/open_llama_3b](https://huggingface.aco/openlm-research/open_llama_3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfdce92",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "The following code sets up the Python environment on the workbench, loads and deploys the model into a model endpoint and provides an example on how to run inference on the deployed model.\n",
    "\n",
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39d39d-7072-491c-9538-45a763d7447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud project id.\n",
    "PROJECT_ID = !gcloud config get project\n",
    "PROJECT_ID = PROJECT_ID.n\n",
    "print(\"Project ID: \" + PROJECT_ID)\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"europe-west2\"\n",
    "print(\"Region: \"+ REGION)\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "BUCKET_URI = \"gs://gen-ai-%s-bucket\" % PROJECT_ID\n",
    "print(\"Bucket URI: \" + BUCKET_URI)\n",
    "\n",
    "import os\n",
    "\n",
    "#Buckets or folders to store required model components\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
    "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
    "\n",
    "# The Service Account for deploying fine tuned model. It requires the `Vertex AI User` and `Storage Object Admin` roles.\n",
    "SERVICE_ACCOUNT = \"%s-consumer-sa@%s.iam.gserviceaccount.com\"  % (PROJECT_ID,PROJECT_ID)\n",
    "print(\"Service Account:\" + SERVICE_ACCOUNT)\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99653fea",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245153eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c6360",
   "metadata": {},
   "source": [
    "### Define Image Location Constants\n",
    "\n",
    "The following constants define the location of the container images to be used in the endpoint to serve requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ddf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prebuilt training and serving Docker images.\n",
    "PREDICTION_DOCKER_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417e55b",
   "metadata": {},
   "source": [
    "### Deploy Prebuilt OpenLLaMA\n",
    "\n",
    "This section deploys prebuilt OpenLLaMA models on the Endpoint. The model deployment step will take ~15 minutes to complete.\n",
    "\n",
    "The peak GPU memory usages for [openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b), [openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b), and [openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b) are ~5.3G, ~8.7G and ~15.2G separately with the default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785acecb",
   "metadata": {},
   "source": [
    "Set the prebuilt model id. For larger versions of the model it may be necessary to increase compute capacity of the endpoint which may incur higher costs.\n",
    "|Models|\n",
    "| :- |\n",
    "| openlm-research/open_llama_3b |\n",
    "| openlm-research/open_llama_7b |\n",
    "| openlm-research/open_llama_13b |\n",
    "\n",
    "\n",
    "NOTE: The prebuilt model weights will be downloaded on the fly from the original location after the deployment succeeds. Thus, an additional 5 minutes of waiting time is needed **after** the above model deployment step succeeds and before you can run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13f6bf-d23e-4a8e-9e29-5bb3aaa2d10a",
   "metadata": {},
   "source": [
    "### Get Pre-Provisioned Endpoint and Define Model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a04ac9-4ae5-4848-9021-f2e891d00105",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_model_id = \"openlm-research/open_llama_3b\"\n",
    "endpoint = aiplatform.Endpoint(\"projects/%s/locations/europe-west2/endpoints/gen-ai-oss-endpoint\" % PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745ae7c-1d32-407b-aa64-d25a513feb6a",
   "metadata": {},
   "source": [
    "### Upload Model Garden Model To Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059ece6-1465-4b52-8f0a-4bfb6b4cbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"openllama-serve\",\n",
    "    serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
    "    serving_container_ports=[7080],\n",
    "    serving_container_predict_route=\"/predictions/peft_serving\",\n",
    "    serving_container_health_route=\"/ping\",\n",
    "    serving_container_environment_variables={\n",
    "        \"BASE_MODEL_ID\": prebuilt_model_id,\n",
    "        \"TASK\": \"causal-language-modeling-lora\",       \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296825e7-5beb-4cb4-8c6a-f58272a2400c",
   "metadata": {},
   "source": [
    "### Deploy Model to Endpoint\n",
    "\n",
    "This function deploys the model to the model endpoint and associates compute resources with it.\n",
    "\n",
    "You can select from several machine and accelerator types. Reference:\n",
    "\n",
    "- Machine Types: https://cloud.google.com/compute/docs/machine-resource\n",
    "- Accelerator Types: https://cloud.google.com/compute/docs/gpus/gpu-regions-zones\n",
    "\n",
    "Please note that accelerators (GPUs) are limited by region and can incur high costs depending on length of deployment and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c98c14-8abe-47ef-a71d-519371127667",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    deploy_request_timeout=1800,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd45ed1",
   "metadata": {},
   "source": [
    "## Inference using deployed model\n",
    "\n",
    "Please wait for around 5 mins after the deployment completes if you face an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\"prompt\": \"Generate a list of ways that makes Earth unique compared to other planets\"},\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions[0]:\n",
    "    print(prediction[\"generated_text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03934c76",
   "metadata": {},
   "source": [
    "### Clean Up Resources\n",
    "\n",
    "Clean up resources after to avoid excess costs. This can also be done from the cloud console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy all models from the endpoint\n",
    "endpoint.undeploy_all()\n",
    "\n",
    "# Delete Models\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589f590-9309-4540-aab5-5dd3e63c757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Endpoint. Note: if you delete your pre-provisioned endpoint use the function below to create and load another one.\n",
    "endpoint.delete(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f8d7d-e0f1-4de2-aa93-cf2c5008a246",
   "metadata": {},
   "source": [
    "### Deploy Additional Endpoints\n",
    "\n",
    "Use the function below to deploy additional endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3427af-7816-40d4-b90a-06ccfdbda5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=f\"gen-ai-oss-endpoint\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
