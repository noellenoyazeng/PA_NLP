{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning for Model Garden (OpenLLaMA 3B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "This notebook demonstrates fine tuning and deploying OpenLLaMA with performance efficient fine tuning libraries (PEFT). and running inference for a sample LLM from Model Garden (OpenLlama 3B). The functions in this notebook can be adapted for other models from Model Garden.\n",
    "\n",
    "[openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "The following code sets up the Python environment on the workbench, loads and deploys the model into a model endpoint and provides an example on how to run inference on the deployed model.\n",
    "\n",
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud project ID.\n",
    "PROJECT_ID = !gcloud config get project\n",
    "PROJECT_ID = PROJECT_ID.n\n",
    "print(\"Project ID: \" + PROJECT_ID)\n",
    "\n",
    "# The region you want to launch jobs in.\n",
    "REGION = \"europe-west2\"\n",
    "print(\"Region: \"+ REGION)\n",
    "\n",
    "# The Cloud Storage bucket for storing experiments output.\n",
    "BUCKET_URI = \"gs://gen-ai-%s-bucket\" % PROJECT_ID\n",
    "print(\"Bucket URI: \" + BUCKET_URI)\n",
    "\n",
    "import os\n",
    "\n",
    "#Buckets or folders to store required model components\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
    "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
    "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
    "print(\"- Staging Bucket: \" + STAGING_BUCKET)\n",
    "print(\"- Experiment Bucket: \" + EXPERIMENT_BUCKET)\n",
    "print(\"- Data Bucket: \" + DATA_BUCKET)\n",
    "print(\"- Model Bucket: \" + MODEL_BUCKET)\n",
    "\n",
    "# The service account for deploying fine tuned model, it requires the `Vertex AI User` and `Storage Object Admin` roles.\n",
    "SERVICE_ACCOUNT = \"%s-consumer-sa@%s.iam.gserviceaccount.com\"  % (PROJECT_ID,PROJECT_ID)\n",
    "print(\"Service Account:\" + SERVICE_ACCOUNT)\n",
    "\n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Location Constants\n",
    "\n",
    "The following constants define the location of the container images to be used in the endpoint to serve requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prebuilt training and serving docker images.\n",
    "TRAIN_DOCKER_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train\"\n",
    ")\n",
    "PREDICTION_DOCKER_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune and Deploy Prebuilt OpenLLaMA\n",
    "\n",
    "This section demonstrates how to fine tune and deploy OpenLLaMA with PEFT LoRA. The model deployment step will take ~15 minutes to complete.\n",
    "\n",
    "The peak GPU memory usages for [openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b), [openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b), and [openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b) are ~5.3G, ~8.7G and ~15.2G separately with the default settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the prebuilt model ID. For larger versions of the model it may be necessary to increase compute capacity of the endpoint and the training job which may incur higher costs.\n",
    "|Models|\n",
    "| :- |\n",
    "| openlm-research/open_llama_3b |\n",
    "| openlm-research/open_llama_7b |\n",
    "| openlm-research/open_llama_13b |\n",
    "\n",
    "\n",
    "NOTE: The prebuilt model weights will be downloaded on the fly from the original location after the deployment succeeds. Thus, an additional 5 minutes of waiting time is needed **after** the above model deployment step succeeds and before you can run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint(\"projects/%s/locations/europe-west2/endpoints/gen-ai-oss-peft-endpoint\" % PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"openlm-research/open_llama_3b\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune\n",
    "\n",
    "Use the Vertex AI SDK to create and run the custom training jobs with Vertex AI Model Garden training images. \n",
    "\n",
    "In order to make the finetuning efficiently, we enabled quantization for loading pretrained models for finetuning LoRA models. Precision options include `\"4bit\"`, `\"8bit\"`, `\"float16\"` (default) and `\"float32\"`, and the precision can be set via `\"--precision_mode\"`. The peak GPU memory usages are ~7G, ~10G and ~16G for finetuning LoRA models for [openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b), [openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b), and [openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b) separately with default training parameters and the example dataset. open_llama_3b and open_llama_7b can be finetuned on 1 V100, and open_llama_13b can be finetuned on 1 A100 (40G)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Finetune Using a Public HuggingFace Dataset\n",
    "\n",
    "*Run either this cell or option 2 as the output model will be overwritten*\n",
    "\n",
    "This example uses the dataset https://huggingface.co/datasets/Abirate/english_quotes. The google provided container uses the HuggingFace Dataset library to load datasets in: https://huggingface.co/docs/datasets/loading. The name of the passed dataset can be changed to any public dataset available within HuggingFace and usable for this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Abirate/english_quotes\"  \n",
    "\n",
    "# Worker pool spec.\n",
    "# Finetunes open_llama_3b.\n",
    "# Change the machine specifications below if larger models or faster training required.\n",
    "# Note that high compute provisioning result in significant costs\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_T4\"\n",
    "replica_count = 1\n",
    "accelerator_count = 1\n",
    "\n",
    "# Setup training job. Runs a job using a custom Google provided container,\n",
    "job_name = \"openllama-3b-PEFT\"\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=causal-language-modeling-lora\",\n",
    "        f\"--pretrained_model_id={base_model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--output_dir={output_dir_gcsfuse}\",\n",
    "        \"--lora_rank=16\",\n",
    "        \"--lora_alpha=32\",\n",
    "        \"--lora_dropout=0.05\",\n",
    "        \"--warmup_steps=10\",\n",
    "        \"--max_steps=10\",\n",
    "        \"--learning_rate=2e-4\",\n",
    "    ],\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    "    environment_variables={\"WANDB_MODE\":\"offline\"}\n",
    ")\n",
    "\n",
    "print(\"Trained models were saved in: \", output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Finetune Using Dataset from a Cloud Storage Bucket\n",
    "\n",
    "*Run either this cell or option 1 as the output model will be overwritten*\n",
    "\n",
    "This example uses the same public dataset but is obtained from a storage bucket within this project. This pattern can be used to train with public datasets from other sources. \n",
    "\n",
    "The storage bucket is mounted to the container running the PEFT job. Data files can be uploaded to the gen-ai storage bucket within the data folder (in structured data formats such as jsonl) and will picked up by the job with this configuration.\n",
    "\n",
    "You can also extract data from sources like Big Query and store it within Cloud Storage to use with this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"quotes.jsonl\"  \n",
    "\n",
    "# Worker pool spec.\n",
    "# Finetunes open_llama_3b.\n",
    "# Change the machine specifications below if larger models or faster training required.\n",
    "# Note that high compute provisioning result in significant costs\n",
    "machine_type = \"n1-standard-8\"\n",
    "accelerator_type = \"NVIDIA_TESLA_T4\"\n",
    "replica_count = 1\n",
    "accelerator_count = 1\n",
    "\n",
    "# Setup training job. Runs a job using a custom Google provided container,\n",
    "job_name = \"openllama-3b-PEFT\"\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "# Note: Mount location of the storage bucket is /gcs/ within the container's filesystem.\n",
    "# The bucket is mounted using Cloud Storage FUSE which allows for the mounted bucket to be interacted with as a filesystem\n",
    "# This is the same way the updates in this notebook gets stored in the user-guide bucket. \n",
    "data_dir = DATA_BUCKET\n",
    "data_dir_gcsfuse = data_dir.replace(\"gs://\", \"/gcs/\") # Sets the folder for the training data (Mounted to GCS)\n",
    "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
    "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\") # Sets the folder for the output model (Mounted to GCS)\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=causal-language-modeling-lora\",\n",
    "        f\"--pretrained_model_id={base_model_id}\",\n",
    "        f\"--dataset_name={data_dir_gcsfuse}\",\n",
    "        f\"--output_dir={output_dir_gcsfuse}\",\n",
    "        \"--lora_rank=16\",\n",
    "        \"--lora_alpha=32\",\n",
    "        \"--lora_dropout=0.05\",\n",
    "        \"--warmup_steps=10\",\n",
    "        \"--max_steps=10\",\n",
    "        \"--learning_rate=2e-4\",\n",
    "    ],\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    "    environment_variables={\"WANDB_MODE\":\"offline\"}\n",
    ")\n",
    "\n",
    "print(\"Trained models were saved in: \", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
    "\n",
    "The model deployment step will take ~15 minutes to complete.\n",
    "\n",
    "The peak GPU memory usages for [openlm-research/open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b), [openlm-research/open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b), and [openlm-research/open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b) with LoRA weights are ~5.3G, ~8.7G and ~15.2G separately with the default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Model Garden Model To Model Registry\n",
    "\n",
    "Note that the serving container environment variables now contains the path to the finetuned LoRA in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"openllama-peft-serve\",\n",
    "    serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
    "    serving_container_ports=[7080],\n",
    "    serving_container_predict_route=\"/predictions/peft_serving\",\n",
    "    serving_container_health_route=\"/ping\",\n",
    "    serving_container_environment_variables={\n",
    "        \"BASE_MODEL_ID\": base_model_id,\n",
    "        \"TASK\": \"causal-language-modeling-lora\",\n",
    "        #This sets the path to the fine-tuned adapter model\n",
    "        \"FINETUNED_LORA_MODEL_PATH\": output_dir\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model to Endpoint\n",
    "\n",
    "This function deploys the model to the model endpoint and associates compute resources with it.\n",
    "\n",
    "You can select from several machine and accelerator types. Reference:\n",
    "\n",
    "- Machine Types: https://cloud.google.com/compute/docs/machine-resource\n",
    "- Accelerator Types: https://cloud.google.com/compute/docs/gpus/gpu-regions-zones\n",
    "\n",
    "Please note that accelerators (GPUs) are limited by region and can incur high costs depending on length of deployment and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    deploy_request_timeout=1800,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using deployed model\n",
    "\n",
    "NOTE: After the deployment succeeds, the base model weights will be downloaded on the fly from the original location and LoRA model weights will be downloaded from the GCS bucket used in training above. Thus, an additional 5 minutes of waiting time is needed **after** the above model deployment step succeeds and before you can run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\"prompt\": \"Generate a list of ways that makes Earth unique compared to other planets\"},\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions[0]:\n",
    "    print(prediction[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Resources\n",
    "\n",
    "Clean up resources after to avoid excess costs, this can also be done from the cloud console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy all models from the endpoint\n",
    "endpoint.undeploy_all()\n",
    "\n",
    "# Delete Models\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Endpoint, Note: if you delete your pre-provisioned endpoint use the function below to create and load another one.\n",
    "endpoint.delete(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Additional Endpoints\n",
    "\n",
    "Use the function below to deploy additional endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=f\"gen-ai-oss-peft-endpoint\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
