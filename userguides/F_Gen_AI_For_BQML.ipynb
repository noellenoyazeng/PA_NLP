{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b496cfd8-f6c4-49ae-ba58-401097f2368e",
   "metadata": {},
   "source": [
    "# Gen AI for BQML\n",
    "This Notebook shows you how to create a BigQuery ML remote model that references a Vertex AI natural language foundation model. You can then use that model in conjunction with the ML.GENERATE_TEXT function to analyze text in a BigQuery table.\n",
    "\n",
    "You can run any of the queries shown here within the Big Query Studio UI **accessible from the link from the cell below or through the cloud console**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20271055-f2e6-4d66-af1f-d093a93373fa",
   "metadata": {},
   "source": [
    "### Pre-Requisites\n",
    "The Generative AI Learning Environment has already been setup with the required connector for Gen AI for BQML and it will be used in the following example code.\n",
    "\n",
    "Import the Big Query Python SDK\n",
    "\n",
    "This cell also generates a direct link to Big Query Studio within this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067bee8-546d-4c20-8f65-20f440f20e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud project id.\n",
    "class format:\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    Default = '\\033[0m'\n",
    "\n",
    "PROJECT_ID = !gcloud config get project\n",
    "PROJECT_ID = PROJECT_ID.n\n",
    "print(\"Project ID: \" + PROJECT_ID + \"\\n\")\n",
    "\n",
    "print(format.BOLD + \"Biq Query UI Link: \" + format.UNDERLINE + \"https://console.cloud.google.com/bigquery?project={}\".format(PROJECT_ID) + format.Default) \n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d605adb-05c7-481c-8faf-f24a8daaa98a",
   "metadata": {},
   "source": [
    "### Create a BQML Model\n",
    "\n",
    "This function creates a remote model using the pre-provisioned remote model connection within this environment, you can set the endpoint field to be any supported generative AI foundation model such as \"code-bison@001\").\n",
    "\n",
    "Reference for Syntax: https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e01573-73f8-4b98-a6df-c99ff62d6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "create_endpoint_connection = \"\"\"\n",
    "    CREATE OR REPLACE MODEL `{}.gen_ai_dataset.textbison-model` REMOTE WITH CONNECTION `{}.europe-west2.bqllm` OPTIONS (ENDPOINT = 'text-bison@001')\n",
    "\"\"\".format(PROJECT_ID,PROJECT_ID)\n",
    "model_create_job = client.query(create_endpoint_connection)  # Make an API request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751f921-27b5-41a1-8882-f31f91040b80",
   "metadata": {},
   "source": [
    "### Simple Generation Query\n",
    "\n",
    "The pre-loaded gen_ai_bqtable contains a list of prompts, the following example generates responses based on those prompts. As mentioned above you can run the query string directly within big-query studio for further experimentation (please make sure to replace the \"{}\" template within the string to the project id of your playpen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27720ed-64ed-4cbe-b955-3a3e0168438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_using_prompts = \"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.GENERATE_TEXT(\n",
    "    MODEL `{}.gen_ai_dataset.textbison-model`,\n",
    "    (\n",
    "    SELECT\n",
    "        CONCAT(string_field_0)\n",
    "        AS prompt from `{}.gen_ai_dataset.gen_ai_bqtable`\n",
    "          limit 5\n",
    "    ),\n",
    "    STRUCT(\n",
    "      0.4 AS temperature, 100 AS max_output_tokens, 0.5 AS top_p,\n",
    "      40 AS top_k, TRUE AS flatten_json_output));\n",
    "\"\"\".format(PROJECT_ID,PROJECT_ID)\n",
    "\n",
    "model_generate_job_simple = client.query(generate_using_prompts)  # Make an API request.\n",
    "\n",
    "print(\"Categories: \\n\")\n",
    "for row in model_generate_job_simple:\n",
    "    # Row values can be accessed by field name or index.\n",
    "    print(\"prompt={} \\n category={} \\n\\n\\n\".format(row[\"prompt\"], row[\"ml_generate_text_llm_result\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc674cc-f921-4815-9936-e19f3f07f99f",
   "metadata": {},
   "source": [
    "### Further Example of a Generation Query\n",
    "Gen AI for BQML can also be used for more complex data analysis tasks such as in the following example. This query uses the same data set of prompts as above and categorisies each sentence into a one word category. This can be used to quickly analyse datasets, for example if you want to extract the programming language of each row of a dataset containing code or for NLP tasks on large datasets such as sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b2e5e-cfee-485c-b2cf-762dd5c3269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorise_prompts = \"\"\"\n",
    "SELECT\n",
    "  ml_generate_text_result['predictions'][0]['content'] AS generated_text,\n",
    "  ml_generate_text_result['predictions'][0]['safetyAttributes']\n",
    "    AS safety_attributes,\n",
    "  * EXCEPT (ml_generate_text_result)\n",
    "FROM\n",
    "  ML.GENERATE_TEXT(\n",
    "    MODEL `{}.gen_ai_dataset.textbison-model`,\n",
    "    (\n",
    "    SELECT\n",
    "        CONCAT('Could you categorise the following generative ai model prompt using only 1 word: ', string_field_0)\n",
    "        AS prompt from `{}.gen_ai_dataset.gen_ai_bqtable`\n",
    "          limit 5\n",
    "    ),\n",
    "    STRUCT(\n",
    "      0.2 AS temperature,\n",
    "      100 AS max_output_tokens))\n",
    "\"\"\".format(PROJECT_ID,PROJECT_ID)\n",
    "\n",
    "model_generate_job = client.query(categorise_prompts)\n",
    "\n",
    "print(\"Categories: \\n\")\n",
    "for row in model_generate_job:\n",
    "    # Row values can be accessed by field name or index.\n",
    "    print(\"prompt={} \\t category={} \\n\".format(row[\"prompt\"], row[\"generated_text\"]))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
